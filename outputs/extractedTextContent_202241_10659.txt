Let me read this for you. . . o e . . Trace-based Just-in-Time Type Specialization for Dynamic Languages Andreas Gal**, Brendan Eich*, Mike Shaver*, David Anderson*, David Mandelin*, Mohammad R. Haghighat$, Blake Kaplan*, Graydon Hoare*, Boris Zbarsky*, Jason Orendorff*, Jesse Ruderman®, Edwin Smith#, Rick Reitmaier®, Michael Bebenita®, Mason Chang"'#, Michael Franzt Mozilla Corporation™ {gal ,brendan, shaver,danderson,dmandelin,mrbkap,graydon,bz, jorendorff ,jruderman}@mozilla. com Adobe Corporation™ {edwsmith,rreitmai}@adobe.com Intel Curpureltiun$ {mohammad.r.haghighat}@intel.com University of California, Irvine™ {mbebenit,changm,franz}Quci.edu Abstract and is used for the application logic of browser-based productivity Dynamic languages such as JavaScript are more difficult to comilpiflwa_tmng sgchlas (kivgul%le Mall’. Gou&gle Docs ‘m:; Z“‘ébfz C(_)l_ pile than statically typed ones. Since no concrete type information a un.mo? u‘;e‘ rtl,lt 15 domain, 1n or efr to lprov1 E_d o luser is available, traditional compilers need to emit generic code that can ezpenence anc epz ¢ il new generation o ddEP L"‘lm}m’ virtual mahandle all possible type combinations at runtime. We present an al¢ 1r(1:es mlfi“ pr?w ca 0\;1/ hldﬁug ilme an l’:l per urm(u.u.;., ternative compilation technique for dynamically-typed languages _ -omptlers Urﬂ;t‘f”"d Y ti;Pe d(r;cuflcezre yon tl}l'pe n me that identifies frequently executed loop traces at run-time and then tion to generate efficient machine code. In a dynamically type _Pm_ generates machine code on the fly that is specialized for the acgramming languz_xge sucl_1 as JavaScript, the types of expressions tual dynamic types occurring on each path through the loop. Our mdy vary at runtime. T_hls‘rr_leans thut_the.z.ompll_er can no longer method provides cheap inter-procedural type specialization, and an easily transf_qrm operations into machme_ instructions that operate elegant and efficient way of incrementally compiling lazily discovon one SPECth type. Wlthout exact type information, the culppller ered alternative paths through nested loops. We have implemented must emit slower generalized machine code that can deal with all a dynamic compiler for JavaScript based on our technique and we potenthl type combinations. While gomplle-t_lme static type mtEF_ have measured speedups of 10x and more for certain benchmark ence might !Je able to gut.h.er type 1_nturmau<_m o generate optie mized machine code, traditional static analysis is very expensive programs 1 h de, traditional stat ly Yy exp = and hence not well suited for the highly interactive environment of Categories and Subject Descriptors D.3.4 [Programming Lana web browser. guages): Processors — Incremental compilers, code generation. ‘We present a trace-based compilation technique for dynamic General Terms Design, Experimentation, Measurement, Perforlfmguages ﬂ?m reconciles SPEEd.Uf compilation with excellem. permance Ten ’ ; ? formance of the generated machine code. Our system uses a mixed. mode execution approach: the system starts running JavaScript in a Keywords  JavaScript, just-in-time compilation, trace trees. fast-starting bytecode interpreter. As the program runs, the system identifies hot (frequently executed) bytecode sequences, records 1 Introduction them, and compiles them to fast native code. We call such a se. . quence of instructions a trace. D)"mf'“f languages such as J“V“SC“HL Python. and Ruby. are popUnlike method-based dynamic compilers, our dynamic comular since they are CXPpressive, u.ccesslble to non-experts, and ma}(&: piler operates at the granularity of individual loops. This design deployment as easy as distributing a source file. They are used for choice is based on the expectation that programs spend most of small scripts as We}l as for cump‘lex u_ppllcz_xtlun& JavaScript, _fur their time in hot loops. Even in dynamically typed languages, we example, is the de facto standard for client-side web programming expect hot loops to be mostly rype-stable, meaning that the types of values are invariant. (12) For example, we would expect loop counters that start as integers to remain integers for all iterations. When both of these expectations hold. a trace-based compiler can cover Permission to make digital or hard copies of all or part of this work for personal or the program execution with a small number of type-specialized, efclassroom use is granted without fee provided that copies are not made or distributed ficiently compiled traces. for profit or commercial advantage and that copies bear this notice and the full citation Each c iled trace ¢ . ath th oh th o ith on the first page. To copy otherwise, (o republish, to post on servers or to redistribute A4 L.Umpl‘e Tace covers one pa roug ¢ program \>_\/l o lists, requires prior specific permission and/or a fee. one mapping of values to types. When the VM executes a compiled PLDI'09, June 15-20, 2009, Dublin, Ireland. trace, it cannot guarantee that the same path will be followed Copyright © 2009 ACM 978-1-60558-392-1/09/06. . . $5.00 or that the same types will occur in subsequent loop iterations.  Hence, recording and compiling a trace speculates that the path and 1 for (var i = 2; i < 100; ++i) { typing will be exactly as they were during recording for subsequent 2 if (tprimes[il) iterations of the loop. 3 continue; Every compiled trace contains all the guards (checks) required 4 for (var k =i + i; i < 100; k += i) to validate the speculation. If one of the guards fails (if control 5 primes(k] = false; flow is different, or a value of a different type is generated), the 6} trace exits. If an exit becomes hot, the VM can record a branch trace starting at the exit to cover the new path. In this way, the VM Figure 1. Sample program: sieve of Eratosthenes. primes is records a trace tree covering all the hot paths through the loop. initialized to an array of 100 false values on entry to this code Nested loops can be difficult to optimize for tracing VMs. In snippet. a naive implementation, inner loops would become hot first, and the VM would start tracing there. When the inner loop exits, the VM would detect that a different branch was taken. The VM would try to record a branch trace, and find that the trace reaches not the Symbol Koy inner loop header, but the outer loop header. At this point, the VM " Overhead could continue tracing until it reaches the inner loop header again, Ilmermn-gl thus tracing the outer loop inside a trace tree for the inner loop. ;%ZZ coldblacklisted But this requires tracing a copy of the outer loop for every side exit loop/exit and type combination in the inner loop. In essence, this is a form abort T enier compiled trace of unintended tail duplication, which can easily overflow the code recording ready cache. Alternatively, the VM could simply stop tracing, and give up oot T Emer Ty on ever tracing outer loops. A Trece Sompled Trece We solve the nested loop problem by recording nested trace oo hoer loop edge with trees. Our system traces the inner loop exactly as the naive version. same types The system stops extending the inner tree when it reaches an outer /" Compilo loop, but then it starts a new trace at the outer loop header. When (LG Sl the outer loop reaches the inner loop header, the system tries to call the trace tree for the inner loop. If the call succeeds, the VM records o :x“":“:;‘:me Y e:::jnle‘::;e the call to the inner tree as part of the outer trace and finishes { Leave Y the outer trace as normal. In this way, our system can trace any PRI number of loops nested to any depth without causing excessive tail duplication. - These techniques allow a VM to dynamically translate a proFigure 2. State machine describing the major activities of Tracegram to nested, type-specialized trace trees. Because traces can Monkey and the conditions that cause transitions to a new activcross function call boundaries, our techniques also achieve the efity. In the dark box, TM executes JS as compiled traces. In the fects of inlining. Because traces have no internal control-flow joins, light gray boxes, TM executes JS in the standard interpreter. White they can be optimized in linear time by a simple compiler (10). boxes are overhead. Thus, to maximize performance. we need to Thus, our tracing VM efficiently performs the same kind of opmaximize time spent in the darkest box and minimize time spent in timizations that would require interprocedural analysis in a static the white boxes. The best case is a loop where the types at the loop optimization setting. This makes tracing an attractive and effective edge are the same as the types on entry—then TM can stay in native tool to type specialize even complex function call-rich code. code until the loop is done. ‘We implemented these techniques for an existing JavaScript interpreter, SpiderMonkey. We call the resulting tracing VM TraceMonkey. TraceMonkey supports all the JavaScript features of Spia set of industry benchmarks. The paper ends with conclusions in derMonkey, with a 2x-20x speedup for traceable programs. Section 9 and an outlook on future work is presented in Section 10. This paper makes the following contributions: . . i . o 2. Overview: Example Tracing Run ® We explain an algorithm for dynamically forming trace trees to . i . . . . cover a program, representing nested loops as nested trace trees. This section provides an overview of our system by describing . . . L how TraceMonkey executes an example program. The example o We explum howito speculauyely generate efficient type-specialized program, shown in Figure 1, computes the first 100 prime numbers code for traces from dynamic language programs. with nested loops. The narrative should be read along with Figure 2, ® We validate our tracing techniques in an implementation based which describes the activities TraceMonkey performs and when it on the SpiderMonkey JavaScript interpreter, achieving 2x-20x transitions between the loops. speedups on many programs. TraceMonkey always begins executing a program in the bytecode interpreter. Every loop back edge is a potential trace point. The remainder of this paper is organized as follows. Section 3 is ‘When the interpreter crosses a loop edge, TraceMonkey invokes a general overview of trace tree based compilation we use to capthe trace monitor, which may decide to record or execute a native ture and compile frequently executed code regions. In Section 4 trace. At the start of execution, there are no compiled traces yet, so we describe our approach of covering nested loops using a numthe trace monitor counts the number of times each loop back edge is ber of individual trace trees. In Section 5 we describe our traceexecuted until a loop becomes hot, currently after 2 crossings. Note compilation based speculative type specialization approach we use that the way our loops are compiled, the loop edge is crossed before to generate efficient machine code from recorded bytecode traces. entering the loop, so the second crossing occurs immediately after Our implementation of a dynamic type-specializing compiler for the first iteration. JavaScript is described in Section 6. Related work is discussed in Here is the sequence of events broken down by outer loop Section 8. In Section 7 we evaluate our dynamic compiler based on iteration:  v0 := 1d state[748] // load primes from the trace activation record st spl[0], vO // store primes to interpreter stack vl := 1d state[764] // load k from the trace activation record v2 := i2f(v1) // convert k from int to double st spl8], vi // store k to interpreter stack st spl[16], 0 // store false to interpreter stack v3 := 1d v0[4] // load class word for primes v4 := and v3, -4 // mask out object class tag for primes v6 := eq v4, Array // test whether primes is an array xf v5 // side exit if vb is false v6 := js_Array_set(v0, v2, false) // call function to set array element v7 := eq v6, O // test return value from call xt v7 // side exit if js_Array_set returns false. Figure 3. LIR snippet for sample program. This is the LIR recorded for line 5 of the sample program in Figure 1. The LIR encodes the semantics in SSA form using temporary variables. The LIR also encodes all the stores that the interpreter would do to its data stack. Sometimes these stores can be optimized away as the stack locations are live only on exits to the interpreter. Finally, the LIR records guards and side exits to verify the assumptions made in this recording: that primes is an array and that the call to set its element succeeds. mov edx, ebx(748) // load primes from the trace activation record mov edi(0), edx // (%) store primes to interpreter stack mov esi, ebx(764) // load k from the trace activation record mov edi(8), esi // (%) store k to interpreter stack mov edi(16), 0 // (%) store false to interpreter stack mov eax, edx(4) // (*) load object class word for primes and eax, -4 // (%) mask out object class tag for primes cmp eax, Array // (*) test whether primes is an array jne side_exit_1 // (%) side exit if primes is not an array sub esp, 8 // bump stack for call alignment convention push false // push last argument for call push esi // push first argument for call call js_Array_set // call function to set array element add esp, 8 // clean up extra stack space mov ecx, ebx // (*) created by register allocator test eax, eax // (%) test return value of js_Array_set je side_exit_2 // (%) side exit if call failed side_exit_1: mov ecx, ebp(-4) // restore ecx mov esp, ebp // restore esp jmp epilog // jump to ret statement Figure 4. x86 snippet for sample program. This is the x86 code compiled from the LIR snippet in Figure 3. Most LIR instructions compile to a single x86 instruction. Instructions marked with (*) would be omitted by an idealized compiler that knew that none of the side exits would ever be taken. The 17 instructions generated by the compiler compare favorably with the 100+ instructions that the interpreter would execute for the same code snippet, including 4 indirect jumps. i=2. This is the first iteration of the outer loop. The loop on interpreter PC and the types of values match those observed when lines 4-5 becomes hot on its second iteration, so TraceMonkey entrace recording was started. The first trace in our example, T}s, ters recording mode on line 4. In recording mode, TraceMonkey covers lines 4 and 5. This trace can be entered if the PC is at line 4, records the code along the trace in a low-level compiler intermedii and k are integers, and primes is an object. After compiling Tss. ate representation we call LIR. The LIR trace encodes all the operTraceMonkey returns to the interpreter and loops back to line 1. ations performed and the types of all operands. The LIR trace also i=3. Now the loop header at line 1 has become hot, so Traceencodes guards, which are checks that verify that the control flow Monkey starts recording. When recording reaches line 4, Traceand types are identical to those observed during trace recording. Monkey observes that it has reached an inner loop header that alThus, on later executions, if and only if all guards are passed, the ready has a compiled trace, so TraceMonkey attempts to nest the trace has the required program semantics. inner loop inside the current trace. The first step is to call the inner TraceMonkey stops recording when execution returns to the trace as a subroutine. This executes the loop on line 4 to completion loop header or exits the loop. In this case, execution returns to the and then returns to the recorder. TraceMonkey verifies that the call loop header on line 4. was successful and then records the call to the inner trace as part of After recording is finished, TraceMonkey compiles the trace to the current trace. Recording continues until execution reaches line native code using the recorded type information for optimization. 1. and at which point TraceMonkey finishes and compiles a trace The result is a native code fragment that can be entered if the for the outer loop, 7.  i=4. On this iteration, TraceMonkey calls 776. Because i=4, the A trace records all its intermediate values in a small activation if statement on line 2 is taken. This branch was not taken in the record area. To make variable accesses fast on trace, the trace also original trace, so this causes 716 to fail a guard and take a side exit. imports local and global variables by unboxing them and copying The exit is not yet hot, so TraceMonkey returns to the interpreter, them to its activation record. Thus, the trace can read and write which executes the continue statement. these variables with simple loads and stores from a native activation i=5. TraceMonkey calls 7’6, which in turn calls the nested trace recording, independently of the boxing mechanism used by the Tus. Tie loops back to its own header, starting the next iteration interpreter. When the trace exits, the VM boxes the values from without ever returning to the monitor. this native storage location and copies them back to the interpreter i=6. On this iteration, the side exit on line 2 is taken again. This structures. time, the side exit becomes hot, so a trace 73,1 is recorded that For every control-flow branch in the source program, the covers line 3 and returns to the loop header. Thus, the end of 7531 recorder generates conditional exit LIR instructions. These instrucjumps directly to the start of 7. The side exit is patched so that tions exit from the trace if required control flow is different from on future iterations, it jumps directly to 753.1. what it was at trace recording, ensuring that the trace instructions At this point, TraceMonkey has compiled enough traces to cover are run only if they are supposed to. We call these instructions the entire nested loop structure, so the rest of the program runs guard instructions. entirely as native code. Most of our traces represent loops and end with the special loop LIR instruction. This is just an unconditional branch to the top of the trace. Such traces return only via guards. 3. Trace Trees Now, we describe the key optimizations that are performed as In this section, we describe traces, trace trees, and how they are part of recording LIR. All of these optimizations reduce complex formed at run time. Although our techniques apply to any dynamic dynamic language constructs to simple typed constructs by spelanguage interpreter, we will describe them assuming a bytecode cializing for the current trace. Each optimization requires guard ininterpreter to keep the exposition simple. structions to verify their assumptions about the state and exit the trace if necessary. 3.1 Traces Type specialization. S . L All LIR primitives apply to operands of specific types. Thus, A trace 1 simply program path, which may cross fum_:u_on call LIR traces fre necessarlijll))/ ytype-slz)ecialized, fnd a cuynlipiler can boundaries. TraceMonkey tuguses on ]0?[7 traces, that originate at easily produce a translation that requires no type dispatches. A a loop edge and represent a single iteration through the associated typical bytecode interpreter carries tag bits along with each value, loup,_ . . . and to perform any operation, must check the tag bits, dynamically Similar to an extended bas{c block, a trace is only entered at dispatch, mask out the tag bits to recover the untagged value, the top, but may have many exits. In contrast to an extended basic perform the operation, and then reapply tags. LIR omits everything block, a trace can contain join nodes. Since a trace always only except the operation itself. = follows one single path through the original program. huweverj Join A potential problem is that some operations can produce values nodes are not recggmzable as such in a trace and have a single of unpredictable types. For example, reading a property from an predecessor node .hke regular nodes. . . . object could yield a value of any type, not necessarily the type . A t)ped frace 1s a trace annotated with a type for every variable observed during recording. The recorder emits guard instructions (including t_el?lpumnes) on the trace. Atype.d trace also has an entry that conditionally exit if the operation yields a value of a different fype map giving the required types for variables used on the trace type from that seen during recording. These guard instructions befon? they are defined. For exum_ple, a trace could have a type map guarantee that as long as execution is on trace, the types of values G int, b: boglean)._ meaning tk}at the. trace may be entergd match those of the typed trace. When the VM observes a side exit fmly if the value of the variable x is of type int m_ld the vu.lue of b along such a type guard, a new typed trace is recorded originating :vf(')lff%,rl:ziizzojiean The entry type map is much like the signature at the side exit location, capturing the new type of the operation in < L . . uestion. .I" this paper. we oan dmcu:s typed loop traces, fmd we will ! Representation specialization: objects. In JavaScript, name refer to them simply as traces . The kgy. pruperty_ut typed 190[1 lookup semantics are complex and potentially expensive because traces is that th_ey can be compiled to efficient machine code using they include features like object inheritance and eval. To evaluate the same techniques used for typed langu.age& an object property read expression like o.x, the interpreter must In Trachonkey,_ traces are recu_rded in trace-flavored SSA LIR search the property map of o and all of its prototypes and parents. (low-level _mtermedlute representation). In tra.ce-ﬂav_ora.i SSA (or Property maps can be implemented with different data structures TSSA), phi nodes appear only at the entry point, which is _rez{c_hed (e.g.. per-object hash tables or shared hash tables). so the search both on entry and via loop edges. The important LIR primitives process also must dispatch on the representation of each object are constant values, memory _loads _and stores (by add_ress and found during search. TraceMonkey can simply observe the result of offset), integer operators, ﬂummg-[_)omt operators, function calls, the search process and record the simplest possible LIR to access and conditional exits. Typ_e conversions, such as integer to double. the property value. For example, the search might finds the value of are representeq by function Fulls‘ This makes the LIR used by o.x in the prototype of o, which uses a shared hash-table represenTraceMgnkey mdependent of the concrete type system a_nd type tation that places x in slot 2 of a property vector. Then the recorded conversion Tules of the soilrce langua_ga _The LIR operations are can generate LIR that reads o.x with just two or three loads: one to generic enough that the backend compiler is language independent. get the prototype, possibly one to get the property value vector, and Figure 3 shuw.s an example LI.R trace. . . one more to get slot 2 from the vector. This is a vast simplification Bytecode interpreters typically represent values 10 a various and speedup compared to the original interpreter code. Inheritance cgmplex data structures .(E‘g" !msh tables) ma boxed format (i.e., relationships and object representations can change during execuW}Fh.zlttuched type tag I.ms)‘ Since a trace is m.tended to represent tion, so the simplified code requires guard instructions that ensure efficient code that ellml_mné_:s all that_ complexity, our traces operthe object representation is the same. In TraceMonkey, objects’ repate on unboxed values in simple variables and arrays as much as possible.  resentations are assigned an integer key called the object shape. Starting a tree. Tree trees always start at loop headers, because Thus, the guard is a simple equality check on the object shape. they are a natural place to look for hot paths. In TraceMonkey, loop Representation specialization: numbers. JavaScript has no headers are easy to detect—the bytecode compiler ensures that a integer type, only a Number type that is the set of 64-bit IEEEbytecode is a loop header iff it is the target of a backward branch. 754 floating-pointer numbers (“doubles”). But many JavaScript TraceMonkey starts a tree when a given loop header has been exeoperators, in particular array accesses and bitwise operators, really cuted a certain number of times (2 in the current implementation). operate on integers, so they first convert the number to an integer, Starting a tree just means starting recording a trace for the current and then convert any integer result back to a double.! Clearly, a point and type map and marking the trace as the root of a tree. Each JavaScript VM that wants to be fast must find a way to operate on tree is associated with a loop header and type map. so there may be integers directly and avoid these conversions. several trees for a given loop header. In TraceMonkey, we support two representations for numbers: Closing the loop. Trace recording can end in several ways. integers and doubles. The interpreter uses integer representations Ideally, the trace reaches the loop header where it started with as much as it can, switching for results that can only be represented the same type map as on entry. This is called a type-stable loop as doubles. When a trace is started, some values may be imported iteration. In this case, the end of the trace can jump right to the and represented as integers. Some operations on integers require beginning. as all the value representations are exactly as needed to guards. For example, adding two integers can produce a value too enter the trace. The jump can even skip the usual code that would large for the integer representation. copy out the state at the end of the trace and copy it back in to the Function inlining. LIR traces can cross function boundaries trace activation record to enter a trace. in either direction, achieving function inlining. Move instructions In certain cases the trace might reach the loop header with a need to be recorded for function entry and exit to copy arguments different type map. This scenario is sometime observed for the first in and return values out. These move statements are then optimized iteration of a loop. Some variables inside the loop might initially be away by the compiler using copy propagation. In order to be able undefined, before they are set to a concrete type during the first loop to return to the interpreter, the trace must also generate LIR to iteration. When recording such an iteration, the recorder cannot record that a call frame has been entered and exited. The frame link the trace back to its own loop header since it is type-unstable. entry and exit LIR saves just enough information to allow the Instead, the iteration is terminated with a side exit that will always intepreter call stack to be restored later and is much simpler than fail and return to the interpreter. At the same time a new trace is the interpreter’s standard call code. If the function being entered recorded with the new type map. Every time an additional typeis not constant (which in JavaScript includes any call by function unstable trace is added to a region, its exit type map is compared to name), the recorder must also emit LIR to guard that the function the entry map of all existing traces in case they complement each is the same. other. With this approach we are able to cover type-unstable loop Guards and side exits. Each optimization described above iterations as long they eventually form a stable equilibrium. requires one or more guards to verify the assumptions made in Finally. the trace might exit the loop before reaching the loop doing the optimization. A guard is just a group of LIR instructions header, for example because execution reaches a break or return that performs a test and conditional exit. The exit branches to a statement. In this case, the VM simply ends the trace with an exit side exit, a small off-trace piece of LIR that returns a pointer to to the trace monitor. a structure that describes the reason for the exit along with the As mentioned previously, we may speculatively chose to repinterpreter PC at the exit point and any other data needed to restore resent certain Number-typed values as integers on trace. We do so the interpreter’s state structures. when we observe that Number-typed variables contain an integer Aborts. Some constructs are difficult to record in LIR traces. value at trace entry. If during trace recording the variable is unexFor example, eval or calls to external functions can change the pectedly assigned a non-integer value, we have to widen the type program state in unpredictable ways, making it difficult for the of the variable to a double. As a result, the recorded trace becomes tracer to know the current type map in order to continue tracing. inherently type-unstable since it starts with an integer value but A tracing implementation can also have any number of other limiends with a double value. This represents a mis-speculation, since tations, e.g.,a small-memory device may limit the length of traces. at trace entry we specialized the Number-typed value to an integer, ‘When any situation occurs that prevents the implementation from assuming that at the loop edge we would again find an integer value continuing trace recording, the implementation aborts trace recordin the variable, allowing us to close the loop. To avoid future specing and returns to the trace monitor. ulative failures involving this variable, and to obtain a type-stable trace we note the fact that the variable in question as been observed 32 Trace Trees to s_ometimes huld“non-in’t.eger values in an advisory data structure which we call the “oracle”. Especially simple loops, namely those where control flow, value ‘When compiling loops. we consult the oracle before specializtypes, value representations, and inlined functions are all invariant, ing values to integers. Speculation towards integers is performed can be represented by a single trace. But most loops have at least only if no adverse information is known to the oracle about that some variation, and so the program will take side exits from the particular variable. Whenever we accidentally compile a loop that main trace. When a side exit becomes hot, TraceMonkey starts a is type-unstable due to mis-speculation of a Number-typed varinew branch trace from that point and patches the side exit to jump able, we immediately trigger the recording of a new trace, which directly to that trace. In this way. a single trace expands on demand based on the now updated oracle information will start with a douto a single-entry, multiple-exit trace tree. ble value and thus become type stable. This section explains how trace trees are formed during execuExtending a tree. Side exits lead to different paths through tion. The goal is to form trace trees during execution that cover all the loop, or paths with different types or representations. Thus, to the hot paths of the program. completely cover the loop, the VM must record traces starting at all side exits. These traces are recorded much like root traces: there is . ) . a counter for each side exit, and when the counter reaches a hotness Arrays are actually worse than this: if the index value is a number, it must threshold, recording starts. Recording stops exactly as for the root be CO“VC"_Cd trom_a double to a string tn_r the property access operator, and trace, using the loop header of the root trace as the target to reach. then to an integer internally to the array implementation. =  Our implementation does not extend at all side exits. It extends only if the side exit is for a control-flow branch, and only if the side exit does not leave the loop. In particular we do not want to extend I a trace tree along a path that leads to an outer loop, because we - I want to cover such paths in an outer tree through tree nesting. 577 el e Anchor /o eranch e 3.3 Blacklisting )/ — Sometimes, a program follows a path that cannot be compiled ’ ’/’ et into a trace, usually because of limitations in the implementation. V - TraceMonkey does not currently support recording throwing and catching of arbitrary exceptions. This design trade off was chosen, because exceptions are usually rare in JavaScript. However, if a program opts to use exceptions intensively, we would suddenly incur a punishing runtime overhead if we repeatedly try to record a trace for this path and repeatedly fail to do so, since we abort R tracing every time we observe an exception being thrown. Figure 5. A tree with two traces, a trunk trace and one branch As aresult, if a hot loop contains traces that always fail, the VM trace. The trunk trace contains a guard to which a branch trace was could potentially run much more slowly than the base interpreter: attached. The branch trace contain a guard that may fail and trigger the VM repeatedly spends time trying to record traces, but is never a side exit. Both the trunk and the branch trace loop back to the tree able to run any. To avoid this problem, whenever the VM is about anchor, which is the beginning of the trace tree. to start tracing, it must try to predict whether it will finish the trace. Our prediction algorithm is based on blacklisting traces that have been tried and failed. When the VM fails to finish a trace starting at a given point, the VM records that a failure has occurred. The i e o - VM also sets a counter so that it will not try to record a trace starting at that point until it is passed a few more times (32 in our implementation). This backoff counter gives temporary conditions that prevent tracing a chance to end. For example, a loop may behave o o o o differently during startup than during its steady-state execution. After a given number of failures (2 in our implementation), the VM closed @) ke tnked ®) ked marks the fragment as blacklisted. which means the VM will never again start recording at that point. == [Teeez] [Teee3] After implementing this basic strategy, we observed that for small loops that get blacklisted, the system can spend a noticeable R i sz amount of time just finding the loop fragment and determining that it has been blacklisted. We now avoid that problem by patching the bytecode. We define an extra no-op bytecode that indicates a loop o . e header. The VM calls into the trace monitor every time the interseing preter executes a loop header no-op. To blacklist a fragment, we L linked  Linked Closed simply replace the loop header no-op with a regular no-op. Thus, (c) the interpreter will never again even call into the trace monitor. There is a related problem we have not yet solved, which occurs _ when a loop meets all of these conditions: Figure 6. We handle type-unstable loops by allowing traces to ) compile that cannot loop back to themselves due to a type mis® The VM can form at least one root trace for the loop. match. As such traces accumulate, we attempt to connect their loop e There is at least one hot side exit for which the VM cannot edges to form groups of trace trees that can execute without having complete a trace. to side-exit to the interpreter to cover odd type cases. This is paro The loop body is short. ncularly important tqr ne5ted trace trees Wl?gre an outer tree tries to call an inner tree (or in this case a forest of inner trees), since inner In this case, the VM will repeatedly pass the loop header, search loops frequently have initially undefined values which change type for a trace, find it, execute it, and fall back to the interpreter. to a concrete value after the first iteration. With a short loop body, the overhead of finding and calling the trace is high, and causes performance to be even slower than the i R i basic interpreter. So far, in this situation we have improved the Fhr(.)ugh the inner loop, {i2, i3, i3, a}. The a symbol is used to implementation so that the VM can complete the branch trace. indicate that the .truce loops bu(.:k the tree anchor. . . But it is hard to guarantee that this situation will never happen. When jcxecunun leaves the inner 1909’ the b_aslc design has_t_wu As future work, this situation could be avoided by detecting and choices. First, the system can st({p tracing a1_1d giveup on comp_llmg blacklisting loops for which the average trace call executes few the outt_ar loop, z:_leurly an L_ll?desuable s_ulunonA The other_ z:h_once is bytecodes before returning to the interpreter. to continue tracing, compiling traces for the outer loop inside the inner loop’s trace tree. . For example, the program might exit at i5 and record a branch 4. Nested Trace Tree Formation trace that incorporates the outer loop: {is,i7,i1,%6, %7, %1, c}. Figure 7 shows basic trace tree compilation (11) applied to a nested Later, the program might take the other branch at iz and then loop where the inner loop contains two paths. Usually, the inner exit, recording another branch trace incorporating the outer loop: loop (with header at i2) becomes hot first, and a trace tree is rooted {7?2, T4,15,07, 01,6, 17,01, (1}‘ Thus, the outer loop is recorded and at that point. For example, the first recorded trace may be a cycle compiled twice, and both copies must be retained in the trace cache.  o Teeecal ' 3 ’/' iz L w) < j ",’ Nested Tree [ ¥ ( Ly L W N/ 8 = ¥ » [ is I . T/' (a) (b) _— Figure 8. Control flow graph of a loop with two nested loops (left) _— and its nested trace tree configuration (right). The outer tree calls Figure 7. Control flow graph of a nested loop with an if statement the two inner nested trace trees and places guards at their side exit inside the inner most loop (a). An inner tree captures the inner locations. loop, and is nested inside an outer tree which “calls” the inner tree. The inner tree returns to the outer tree once it exits along its loop condition guard (b). loop is entered with m different type maps (on geometric average), then we compile O(m") copies of the innermost loop. As long as In general, if loops are nested to depth k, and each loop has n paths mis cl_ose to 1, the re_st{ltmg trace trees W'”_ be tractable. (on geometric average), this naive strategy yields O(n*) traces, . An-m1p(_)nam det?ll l‘.“hm the call to the inner trace tree mustact which can easily fill the trace cache. like a funz:t{on cgll site: it must return to the same point every time. In order to execute programs with nested loops efficiently, a The goal of nesting is to r_nake mnner and outer loops mdepende_m: tracine system needs a technique for covering the nested loops with thus when the inner tree is called. it must exit to the same point g sy q g P : : B native code without exponential trace duplication. in the outer tree every time .W"h the same type map. Becm_lse we cannot actually guarantee this property, we must guard on it after 4.1 Nesting Algorithm the call, and side exit if the property does not hold. A common L A 3 3 i reason for the inner tree not to return to the same point would The key msnghf is that if each loop is rep.resemed b_y 1ts own trace be if the inner tree took a new side exit for which it had never tree, the code for euch_luop can be z.:untamed only in its own tree, compiled a trace. At this point, the interpreter PC is in the inner and outer loop Paths v‘_"ll not be duplicated. A_nuther key factis that tree, so we cannot continue recording or executing the outer tree. we are not tracing arbitrary bytecodes that might have 1rreducea.ble If this happens during recording, we abort the outer trace, to give (funtrol flow grap_hs, but rather bytecodes produced F’y a compiler the inner tree a chance to finish growing. A future execution of the for a language with structu_red cuntru_l flow. Thus, given two loop outer tree would then be able to properly finish and record a call to edges, Fhe system can easily d_etermme whether they are nested the inner tree. If an inner tree side exit happens during execution of and W_hl‘fh is the inner loop. Using this knowledge, the system can a compiled trace for the outer tree, we simply exit the outer trace compile inner z_md outer loops separately, and make the outer loop’s and start recording a new branch in the inner tree. traces call the inner loop’s trace tree. ‘ The a_lgorithm for buildi‘ng nested trace trees _is as f_oll(n‘vs; We 42 Blacklisting with Nesting start tracing at loop headers exactly as in the basic tracing system. When we exit a loop (detected by comparing the interpreter PC The blacklisting algorithm needs modification to work well with with the range given by the loop edge), we stop the trace. The nesting. The problem is that outer loop traces often abort during key step of the algorithm occurs when we are recording a trace startup (because the inner tree is not available or takes a side exit), for loop Lz (R for loop being recorded) and we reach the header which would lead to their being quickly blacklisted by the basic of a different loop Lo (O for other loop). Note that Lo must be an algorithm. inner loop of Lz because we stop the trace when we exit a loop. The key observation is that when an outer trace aborts because . . . the inner tree is not ready, this is probably a temporary condition. ¢ If Lo has a type-mafchmg compiled trace tree, we call Lo as Thus, we should not cuuit such ulfons m\{«ard blfcklis}t/ing as long a nested trace tree. It the‘f call succeet.is, then we rECQrd the C%” as we are able to build up more traces for the inner tree. in the tr{lce for L. Qn future executions, the trace for L will In our implementation, when an outer tree aborts on the inner call the inner trace directly. tree, we increment the outer tree’s blacklist counter as usual and e If Lo does not have a type-matching compiled trace tree yet, back off on compiling it. When the inner tree finishes a trace, we we have to obtain it before we are able to proceed. In order decrement the blacklist counter on the outer loop, “forgiving” the to do this, we simply abort recording the first trace. The trace outer loop for aborting previously. We also undo the backoff so that monitor will see the inner loop header, and will immediately the outer tree can start immediately trying to compile the next time start recording the inner loop. ? we reach it. If all the loops in a nest are type-stable, then loop nesting creates no duplication. Otherwise, if loops are nested to a depth £, and each 5. Trace Tree Optimization 2 Instead of aborting the outer recording, we could principally merely susThl_s §ectlon eXPI‘“"S how a recorded trace 1S tr.ansluted to an pend the recording, but that would require the implementation to be able optimized machine code trace. The trace compilation subsystem, to record several traces simultancously, complicating the implementation, NANOJIT, is separate from the VM and can be used for other while saving only a few iterations in the interpreter. applications.  5.1 Optimizations Tag | JS Type Description Because traces are in SSA form and have no join points or ¢xxl nu_mber 31?b1t integer representation - oLt . 000 | object pointer to JSObject handle nodes, certain optimizations are easy to implement. In order to . ) . AR . 010 | number pointer to double handle get good startup performance, the optimizations must run quickly, . . . - o § ! C e - 100 | string pointer to JSString handle so we chose a small set of optimizations. We implemented the e S . . ST - . . 110 | boolean enumeration for null, undefined, true, false optimizations as pipelined filters so that they can be turned on and null. or off independently, and yet all run in just two loop passes over the T . - ; undefined trace: one forward and one backward. Ev_ery time the trace recorder emits a-LIR‘ instruction, the inFigure 9. Tagged values in the SpiderMonkey JS interpreter. struction is immediately passed to the first filter in the forward Testing tags, unboxing (extracting the untagged value) and boxing pipeline. Thus, forward filter optimizations are performed as the (creating tagged values) are significant costs. Avoiding these costs trace is recorded. Ea.ch ﬁlte_r may pass each instruction to the next is a key benefit of tracing. filter unchanged, write a different instruction to the next filter, or write no instruction at all. For example, the constant folding filter can replace a multiply instruction like v13 := mul3, 1000 with a constant instruction v13 = 3000. heuristic selects v with minimum v,,. The motivation is that this We currently apply four forward filters: frees up a register for as long as possible given a single spill. . . o . . . If we need to spill a value v, at this point, we generate the * On ISAs without floating-point instructions, a soft-float filter restore code just after the code for the current instruction. The converts floating-point LIR instructions to sequences of integer corresponding spill code is generated just after the last point where instructions. vs was used. The register that was assigned to v is marked free for © CSE (constant subexpression elimination), the preceding code, because that register can now be used freely . L . - . without affecting the following code ® expression simplification, including constant folding and a few © algebraic identities (e.g., a — a = 0), and oure . o epecific i i s 6. Implementation e source language semantic-specific expression simplification, primarily algebraic identities that allow DOUBLE to be replaced To demonstrate the effectiveness of our approach, we have imwith INT. For example, LIR that converts an INT to a DOUBLE plemented a trace-based dynamic compiler for the SpiderMonkey and then back again would be removed by this filter. JavaScript Virtual Machine (4). SpiderMonkey is the JavaScript o . VM embedded in Mozilla’s Firefox open-source web browser (2), Wk_le" trace recording is cumplete}l, nanojit runs the backwu_rd which is used by more than 200 million users world-wide. The core optimization filters. These_are used for optimizations that require of SpiderMonkey is a bytecode interpreter implemented in C++. backy\_/urd program un_alysns, .When running the backward filters, In SpiderMonkey, all JavaScript values are represented by the nanojit reads one LIR instruction at a time, and the reads are passed type jsval. A jsval is machine word in which up to the 3 of the through the pipeline. . least significant bits are a type tag, and the remaining bits are data. We currently apply three backward filters: See Figure 6 for details. All pointers contained in jsvals point to Lo GC-controlled blocks aligned on 8-byte boundaries. e Dead data-stack store elimination. The LIR trace encodes many - . © Y o . . . JavaScript object values are mappings of string-valued property stores to locations in the interpreter stack. But these values are o o . . g . . names to arbitrary values. They are represented in one of two ways never read back before exiting the trace (by the interpreter or . . ) L ) ) ) = - in SpiderMonkey. Most objects are represented by a shared strucanother trace). Thus, stores to the stack that are overwritten . S . . N . ” tural description, called the object shape, that maps property names before the next exit are dead. Stores to locations that are off N o L . t . o ) - . to array indexes using a hash table. The object stores a pointer to the top of the interpreter stack at future exits are also dead. s ) : . the shape and the array of its own property values. Objects with ® Dead call-stack store elimination. This is the same optimization large. unique sets of property names store their properties directly as above, except applied to the interpreter’s call stack used for in a hash table. function call inlining. The garbage collector is an exact, non-generational, stop-the® Dead code elimination. This eliminates any operation that world mark-and-sweep c.ollectur,. stores to a value that is never used. I_n the rest of t.hls section we discuss key areas of the TraceMonkey implementation. After a LIR instruction is successfully read (“pulled”) from the backward filter pipeline, nanojit’s code generator emits native 6.1 Calling Compiled Traces machine instruction(s) for it. Compiled traces are stored in a trace cache, indexed by intepreter . . PC and type map. Traces are compiled so that they may be 52 Register Allocation called as functions using standard native calling conventions (e.g., We use a simple greedy register allocator that makes a single FASTCALL on x86). backward pass over the trace (it is integrated with the code genThe interpreter must hit a loop edge and enter the monitor in erator). By the time the allocator has reached an instruction like order to call a native trace for the first time. The monitor computes vg = add vy, v2, it has already assigned a register to vs. If v; and the current type map, checks the trace cache for a trace for the vz have not yet been assigned registers, the allocator assigns a free current PC and type map, and if it finds one, executes the trace. register to each. If there are no free registers, a value is selected for To execute a trace, the monitor must build a trace activation spilling. We use a class heuristic that selects the “oldest” registerrecord containing imported local and global variables, temporary carried value (6). stack space, and space for arguments to native calls. The local and The heuristic considers the set 12 of values v in registers immeglobal values are then copied from the interpreter state to the trace diately after the current instruction for spilling. Let v,,, be the last activation record. Then, the trace is called like a normal C function instruction before the current where each v is referred to. Then the pointer.  When a trace call returns, the monitor restores the interpreter Recording is activated by a pointer swap that sets the interstate. First, the monitor checks the reason for the trace exit and preter’s dispatch table to call a single “interrupt” routine for evapplies blacklisting if needed. Then., it pops or synthesizes interery bytecode. The interrupt routine first calls a bytecode-specific preter JavaScript call stack frames as needed. Finally, it copies the recording routine. Then, it turns off recording if necessary (e.g.. imported variables back from the trace activation record to the inthe trace ended). Finally, it jumps to the standard interpreter byteterpreter state. code implementation. Some bytecodes have effects on the type map At least in the current implementation, these steps have a nonthat cannot be predicted before executing the bytecode (e.g., callnegligible runtime cost, so minimizing the number of interpretering String.charCodeAt, which returns an integer or NaN if the to-trace and trace-to-interpreter transitions is essential for perforindex argument is out of range). For these, we arrange for the intermance. (see also Section 3.3). Our experiments (see Figure 12) preter to call into the recorder again after executing the bytecode. show that for programs we can trace well such transitions hapSince such hooks are relatively rare, we embed them directly into pen infrequently and hence do not contribute significantly to total the interpreter, with an additional runtime check to see whether a runtime. In a few programs, where the system is prevented from recorder is currently active. recording branch traces for hot side exits by aborts, this cost can ‘While separating the interpreter from the recorder reduces indirise to up to 10% of total execution time. vidual code complexity. it also requires careful implementation and extensive testing to achieve semantic equivalence. 6.2 Trace Stitching In some cases achieving this equivalence is difficult since SpiTransitions from a trace to a branch trace at a side exit avoid the derMonkey follows a fat-bytecode design, which was found to be costs of calling traces from the monitor, in a feature called trace beneficial to pure interpreter performance. . stitching. At a side exit, the exiting trace only needs to write live In fat-bytecode designs, individual bytecodes can implement register-carried values back to its trace activation record. In our imcomplex processing (e.g., the getprop bytecode, which impleplementation, identical type maps yield identical activation record ments full JavaScript property value access, including special cases layouts, so the trace activation record can be reused immediately for cached and dense array access). ) by the branch trace. Fat _bytecodes have tvt/o udvemtuges:.fewer bytec:udes means In programs with branchy trace trees with small traces, trace lower_dlspmch cost, und.b.lgger byt_ecgde 1mplementzmons give the stitching has a noticeable cost. Although writing to memory and compiler more opportunities to optimize the interpreter. then soon reading back would be expected to have a high L1 Fat bytecodes are a problem for TraceMonkey because they cache hit rate, for small traces the increased instruction count has require the recorder to reimplement the same special case logic a noticeable cost. Also, if the writes and reads are very close in the same way. Also, the advantages are reduced because (a) in the dynamic instruction stream, we have found that current dispatch costs are eliminated entirely in compiled truces,.(b) the x86 processors often incur penalties of 6 cycles or more (e.g., if traces contain only one special case, not the interpreter’s large the instructions use different base registers with equal values, the chunk of code, and (c) TraceMonkey spends less time running the processor may not be able to detect that the addresses are the same base interpreter. . i i i right away). O.ne way we have mltlga.ted these problems is by lmpleme_mmg The alternate solution is to recompile an entire trace tree, thus certain complex bytecodes in the recorder as sequences of simple achieving inter-trace register allocation (10). The disadvantage is bytecodes. Expressing the original semantics this way is not too difthat tree recompilation takes time quadratic in the number of traces. ficult, and recording simple bytecodes is much easier. This enables We believe that the cost of recompiling a trace tree every time us to retain the advantages of fat bytecodes while avoiding some of a branch is added would be prohibitive. That problem might be theu problems for trace recurdlqu This is pamcularly effective for mitigated by recompiling only at certain points, or only for very fat bytecodes _that recurse dek into the interpreter, for example to hot, stable trees. convert an object into a primitive value by invoking a well-known In the future, multicore hardware is expected to be common, methgd on the object, since it lets us _inl‘ine this funz_:tiun c_allA making background tree recompilation attractive. In a closely reIt is important to note that we split fat opcodes into thinner oplated project (13) background recompilation yielded speedups of codes only during recording. When running purely interpretatively up to 1.25x on benchmarks with many branch traces. We plan to (}f_z code that has beer} blacklisted), the interpreter directly and efapply this technique to TraceMonkey as future work. ficiently executes the fat opcodes. 6.3 Trace Recording The job of the trace recorder is to emit LIR with identical semantics 6.4 Preemption to the currently running interpreter bytecode trace. A good impleSpiderMonkey, like many VMs, needs to preempt the user program mentation should have low impact on non-tracing interpreter perperiodically. The main reasons are to prevent infinitely looping formance and a convenient way for implementers to maintain sescripts from locking up the host system and to schedule GC. mantic equivalence. In the interpreter, this had been implemented by setting a “preIn our implementation, the only direct modification to the interempt now” flag that was checked on every backward jump. This preter is a call to the trace monitor at loop edges. In our benchmark strategy carried over into TraceMonkey: the VM inserts a guard on results (see Figure 12) the total time spent in the monitor (for all the preemption flag at every loop edge. We measured less than a activities) is usually less than 5%. so we consider the interpreter 1% increase in runtime on most benchmarks for this extra guard. impact requirement met. Incrementing the loop hit counter is exIn practice, the cost is detectable only for programs with very short pensive because it requires us to look up the loop in the trace cache, loops. but we have tuned our loops to become hot and trace very quickly We tested and rejected a solution that avoided the guards by (on the second iteration). The hit counter implementation could be compiling the loop edge as an unconditional jump, and patching improved, which might give us a small increase in overall perforthe jump target to an exit routine when preemption is required. mance, as well as more flexibility with tuning hotness thresholds. This solution can make the normal case slightly faster, but then Once a loop is blacklisted we never call into the trace monitor for preemption becomes very slow. The implementation was also very that loop (see Section 3.3). complex, especially trying to restart execution after the preemption.  6.5 Calling External Functions . ‘ P - Like most interpreters, SpiderMonkey has a foreign function interi ;m:(a:::d e face (FFI) that allows it to call C builtins and host system functions sring-asta (15| — (e.g.. web browser control and DOM access). The FFI has a stane e dard signature for JS-callable functions, the key argument of which mathspectaknorm (7.1 is an array of boxed values. External functions called through the s e FFI interact with the program state through an interpreter API (e.g., e — - L cateormatiotie (1) ] to read a property from an argument). There are also certain intere, e/ preter builtins that do not use the FFI, but interact with the program ervptomds (23 I'_____ . . C E—— state in the same way, such as the CallIteratorNext function Ry ———————— used with iterator objects. TraceMonkey must support this FFI in pitops nsieve-oits (27— y . ; L itops-ituise-2nd (25,24 order to speed up code that interacts with the host system inside hot :ths—hwts—m—kzne 5 loops. itops Sbitbitsin-byte (25.5) Calling external functions from TraceMonkey is potentially dif:;:S:EYS :i |, ficult because traces do not update the interpreter state until exitez 22— . . S sccessbinanytrees 0.9 [————————— "~~~ ing. In particular, external functions may need the call stack or the sd»:»,ra(s(l S — global variables, but they may be out of date. semeren (2.2 — o . - 3d-cube (2200 [ For the out-of-date call stack problem, we refactored some of the interpreter API implementation functions to re-materialize the O A0% 0% 30%  40% S0% 0% 70% E0%  90% 100% interpreter call stack on demqnd ) ) Dinterpret — We developed a C++ static analysis and annotated some interpreter functions in order to verify that the call stack is refreshed at any point it needs to be used. In (_)rder to access the call stack, Figure 11. Fraction of dynamic bytecodes executed by intera function must be annotated as either FORCESSTACK or RE. } . N o . Lo preter and on native traces. The speedup vs. interpreter is shown QUIRESSTACK. These annotations are also required in order to call s heses ach test. The fracti £ bytecodes REQUIRESSTACK functions, which are presumed to access the call m pdrent_eses next' to eac test. The mcnpn of bytecodes exetack itively. FORC S e P § d o licd cuted while recording is too small to see in this figure, except stac ltr;n;m\'? Y(;R(‘ES TAChK ;5 a t.rustef d"l:l()t‘llt:on" ﬁpp “:i( for crypto-md5, where fully 3% of bytecodes are executed while ;)un) E“S“'“um".tv at med_ns‘t j unction re rl:” es U e_"‘; sft‘“' C recording. In most of the tests, almost all the bytecodes are exeREQUIRES IT/?)CK l;l d;.?“;]'u“ﬁl d"“ﬁ;‘mmi ¢ ‘l; n:)edns tfe llim'icuted by compiled traces. Three of the benchmarks are not traced tion may only be called if the call stack has already been refreshed. at all and run in the interpreter. Similarly, we detect when host functions attempt to directly read or write global variables, and force the currently running trace to sl(g: e‘x.lL Tl;lls h 'r{egessal_r)( SI(TC:: e Cz_ld_le und‘ u.nbux global loops and heavily branching code, and a specialized fuzz tester invariables into the activation record during trace execution. deed revealed several regressions which we subsequently corrected. Since both call-stack access and global variable access are rarely performed by host functions, performance is not significantly . affected by these safety mechanisms. 7. Evaluation Another problem is that external functions can reenter the interWe evaluated our JavaScript tracing implementation using Sunpreter by calling scripts, Whl‘-‘h In turn again ff‘lght want to access Spider, the industry standard JavaScript benchmark suite. SunSpithe call stack or global variables. To address this problem, we made der consists of 26 short-running (less than 250ms, average 26ms) the VM set a flag whenever the interpreter is reentered while a comJavaScript programs. This is in stark contrast to benchmark suites piled trace is running. o i i such as SpecJVMO8 (3) used to evaluate desktop and server Java Every call to an external function then checks this flag and exits VMs. Many programs in those benchmarks use large data sets and the trace immediately after returning from the external function call execute for minutes. The SunSpider programs carry out a variety of if it is set. There are many external functions that seldom or never tasks, primarily 3d rendering, bit-bashing, cryptographic encoding, reenter, and they can be called without problem, and will cause math kernels, and string processing. trace exit only if necessary. . i All experiments were performed on a MacBook Pro with 2.2 The FFI's boxed value array requirement has a performance GHz Core 2 processor and 2 GB RAM running MacOS 10.5. cost, so we defined a new FFI that allows C functions to be anBenchmark results. The main question is whether programs notated with their argument types so that the tracer can call them run faster with tracing. For this, we ran the standard SunSpider test directly. without unnecessary argument Conversions. driver, which starts a JavaScript interpreter, loads and runs each Currently, we do not support calling native property get and set program once for warmup, then loads and runs each program 10 f)vemde functions or DOM functions directly from trace. Support times and reports the average time taken by each. We ran 4 differis planned future work. ent configurations for comparison: (a) SpiderMonkey, the baseline . interpreter, (b) TraceMonkey, (d) SquirrelFish Extreme (SFX), the 6.6 Correctness call-threaded JavaScript interpreter used in Apple’s WebKit, and During development, we had access to existing JavaScript test (e) V8, the method-compiling JavaScript VM from Google. suites, but most of them were not designed with tracing VMs in Figure 10 shows the relative speedups achieved by tracing, SFX, mind and contained few loops. and V8 against the baseline (SpiderMonkey). Tracing achieves the One tool that helped us greatly was Mozilla’s JavaScript fuzz best speedups in integer-heavy benchmarks, up to the 25x speedup tester, JSSFUNFUZZ, which generates random JavaScript programs on bitops-bitwise-and. by nesting random language elements. We modified JSFUNFUZZ TraceMonkey is the fastest VM on 9 of the 26 benchmarks to generate loops, and also to test more heavily certain constructs (3d-morph, bitops-3bit-bits-in-byte, bitops-bitwisewe suspected would reveal flaws in our implementation. For examand, crypto-shal, math-cordic, math-partial-sums, mathple. we suspected bugs in TraceMonkey’s handling of type-unstable spectral-norm, string-base64, string-validate-input).  25 I 1 - $ Tracing 20 1 3 SFX i 3 V8 15 1 1 10 o 1 2 5 I 8 1 = B EE w 1 a3l 13 B 0 L @ E XN @@ DO D O 2 > R Qg bo;o(\\&Q A’“@O & &"‘)(' *o°b & A P \\}%\A & &5 +<5°§ go‘b S oé@q’bo q,‘ze‘b @ O & \QQQ OV &0 &I F T N NG P S S & (\"’\e @ é*Q LSS S L L L& & &L & 5 ;}"\ Q:;o fz;"c @(5, & :.}° X Q‘; ro\ S & o ,,\o« & & g ';é\ & & & 4"}\ S O K RSN & & N E & & & @ @ 2 O O O > A > X P& < g X » & < X QO & F T O ¥ ¥ &8 & && <& & 3 > Figure 10. Speedup vs. a baseline JavaScript interpreter (SpiderMonkey) for our trace-based JIT compiler, Apple’s SquirrelFish Extreme inline threading interpreter and Google’s V8 JS compiler. Our system generates particularly efficient code for programs that benefit most from type specialization, which includes SunSpider Benchmark programs that perform bit manipulation. We type-specialize the code in question to use integer arithmetic, which substantially improves performance. For one of the benchmark programs we execute 25 times faster than the SpiderMonkey interpreter, and almost 5 times faster than V8 and SFX. For a large number of benchmarks all three VMs produce similar results. We perform worst on benchmark programs that we do not trace and instead fall back onto the interpreter. This includes the recursive benchmarks access-binary-trees and control-flow-recursive, for which we currently don’t generate any native code. In particular, the bitops benchmarks are short programs that per® Two programs trace well, but have a long compilation time. form many bitwise operations, so TraceMonkey can cover the enaccess-nbody forms a large number of traces (81). crypto-md5 tire program with 1 or 2 traces that operate on integers. TraceMonforms one very long trace. We expect to improve performance key runs all the other programs in this set almost entirely as native on this programs by improving the compilation speed of nanocode. jit. hf;g_g}_{p—?.na ' (:Um“}i";(l]&y bregul‘ar _FTXFESSIIOH matcl‘ﬂ_ng. ® Some programs trace very well, and speed up compared to \iv 1 'lm l?ﬁ e‘men‘t‘ef:( ma N k: vid SP}:‘UJ kﬂ;guv Tr Fixprel»{on the interpreter, but are not as fast as SFX and/or V8, namely w"kllpl er: ‘us, PT ormance O?htdlﬁ . evn‘(,dn_mrh. _‘15 ittle relation bitops-bits-in-byte, bitops-nsieve-bits, accesso tTe ":d§ (.okmp} vdt‘lunlilppforll-d 1?"“”; m;l 1sbp(1pin ks fannkuch, access-nsieve, and crypto-aes. The reason is b m.;e uin ey; smg Ef;?# ups on the other benchmarks can not clear, but all of these programs have nested loops with ¢ attributed to a few specific causes: small bodies, so we suspect that the implementation has a relatively high cost for calling nested traces. string-fasta traces o The implementation does not currently trace recursion, so well, but its run time is dominated by string processing builtins, TraceMonkey achieves a small speedup or no speedup on which are unaffected by tracing and seem to be less efficient in benchmarks that use recursion extensively: 3d-cube, 3dSpiderMonkey than in the two other VMs. raytrace, access-binary-trees, string-tagcloud, and ) . controlflow-recursive. Detailed performance metrics. In Figure 11 we show the fraco The impl ion does . | . 1 and s tion of instructions interpreted and the fraction of instructions exeh"mt}P e‘"?em‘dt_l"“l( 0es "0; f.ur(r;.mBy ‘tmcve zva ‘lfn some cuted as native code. This figure shows that for many programs, we (;t fetr L'm(,ingn; lﬂjfp emetnte( mb ‘o ev"‘l‘L}?P;_ é:’fe _ z_)rnizt_ are able to execute almost all the code natively. ot T ‘m(v a ? orma . xl};ar use such tunctions in ther Figure 12 breaks down the total execution time into four activmain loops, we do not trace them. ities: interpreting bytecodes while not recording, recording traces e The implementation does not currently trace through regular (including time taken to interpret the recorded trace), compiling expression replace operations. The replace function can be traces to native code, and executing native code traces. passed a function object used to compute the replacement text. These detailed metrics allow us to estimate parameters for a Our implementation currently does not trace functions called simple model of tracing performance. These estimates should be as replace functions. The run time of string-unpack-code is considered very rough, as the values observed on the individual dominated by such a replace call. benchmarks have large standard deviations (on the order of the  Loops Trees Traces Aborts Flushes Trees/Loop Traces/Tree  Traces/Loop  Speedup 3d-cube 25 27 29 3 0 1.1 1.1 1.2 2.20x 3d-morph 5 8 8 2 0 1.6 1.0 1.6 2.86x 3d-raytrace 10 25 100 10 1 25 4.0 10.0 1.18x access-binary-trees 0 0 0 5 0 - - - 0.93x access-fannkuch 10 34 57 24 0 34 1.7 5.7 2.20x access-nbody 8 16 18 5 0 2.0 1.1 23 4.19x access-nsieve 3 6 8 3 0 2.0 1.3 2.7 3.05x bitops-3bit-bits-in-byte 2 2 2 0 0 1.0 1.0 1.0 25.47x bitops-bits-in-byte 3 3 4 1 0 1.0 1.3 1.3 8.67x bitops-bitwise-and 1 1 1 0 0 1.0 1.0 1.0 25.20x bitops-nsieve-bits 3 3 5 0 0 1.0 1.7 1.7 2.75x controlflow-recursive 0 0 0 1 0 - - - 0.98x crypto-aes 50 72 78 19 0 1.4 1.1 1.6 1.64x crypto-md5 4 4 5 0 0 1.0 1.3 1.3 2.30x crypto-shal 5 5 10 0 0 1.0 2.0 2.0 5.95x date-format-tofte 3 3 4 7 0 1.0 1.3 1.3 1.07x date-format-xparb 3 3 11 3 0 1.0 3.7 3.7 0.98x math-cordic 2 4 5 1 0 2.0 1.3 2.5 4.92x math-partial-sums 2 4 4 1 0 2.0 1.0 2.0 5.90x math-spectral-norm 15 20 20 0 0 1.3 1.0 1.3 7.12x regexp-dna 2 2 2 0 0 1.0 1.0 1.0 4.21x string-base64 3 5 7 0 0 1.7 1.4 23 2.53x string-fasta 5 11 15 6 0 22 1.4 3.0 1.49x string-tagcloud 3 6 6 5 0 2.0 1.0 2.0 1.09x string-unpack-code 4 4 37 0 0 1.0 9.3 9.3 1.20x string-validate-input 6 10 13 1 0 1.7 1.3 22 1.86x Figure 13. Detailed trace recording statistics for the SunSpider benchmark set. mean). We exclude regexp-dna from the following calculations, fastest available JavaScript inline threaded interpreter (SFX) on 9 because most of its time is spent in the regular expression matcher, of 26 benchmarks. which has much different performance characteristics from the other programs. (Note that this only makes a difference of about 10% in the results.) Dividing the total execution time in processor clock cycles by the number of bytecodes executed in the base 8. Related Work interpreter shows that on average, a bytecode executes in about Trace optimization for dynamic languages. The closest area of 35 cycles. Native traces take about 9 cycles per bytecode, a 3.9x related work is on applying trace optimization to type-specialize speedup over the interpreter. dynamic languages. Existing work shares the idea of generating Using similar computations, we find that trace recording takes type-specialized code speculatively with guards along interpreter about 3800 cycles per bytecode, and compilation 3150 cycles per traces. bytecode. Hence, during recording and compiling the VM runs at To our knowledge, Rigo’s Psyco (16) is the only published 1/200 the speed of the interpreter. Because it costs 6950 cycles to type-specializing trace compiler for a dynamic language (Python). compile a bytecode, and we save 26 cycles each time that code is Psyco does not attempt to identify hot loops or inline function calls. run natively, we break even after running a trace 270 times. Instead, Psyco transforms loops to mutual recursion before running The other VMs we compared with achieve an overall speedup and traces all operations. of 3.0x relative to our baseline interpreter. Our estimated native Pall’s LuaJIT is a Lua VM in development that uses trace comcode speedup of 3.9x is significantly better. This suggests that pilation ideas. (1). There are no publications on LuaJIT but the creour compilation techniques can generate more efficient native code ator has told us that LuaJIT has a similar design to our system, but than any other current JavaScript VM. will use a less aggressive type speculation (e.g., using a floatingThese estimates also indicate that our startup performance could point representation for all number values) and does not generate be substantially better if we improved the speed of trace recording nested traces for nested loops. and compilation. The estimated 200x slowdown for recording and General trace optimization. General trace optimization has compilation is very rough, and may be influenced by startup factors a longer history that has treated mostly native code and typed in the interpreter (e.g., caches that have not warmed up yet during languages like Java. Thus, these systems have focused less on type recording). One observation supporting this conjecture is that in specialization and more on other optimizations. the tracer, interpreted bytecodes take about 180 cycles to run. Still, Dynamo (7) by Bala et al, introduced native code tracing as a recording and compilation are clearly both expensive, and a better replacement for profile-guided optimization (PGO). A major goal implementation, possibly including redesign of the LIR abstract was to perform PGO online so that the profile was specific to syntax or encoding, would improve startup performance. the current execution. Dynamo used loop headers as candidate hot Our performance results confirm that type specialization using traces, but did not try to create loop traces specifically. trace trees substantially improves performance. We are able to Trace trees were originally proposed by Gal et al. (11) in the outperform the fastest available JavaScript compiler (V8) and the context of Java, a statically typed language. Their trace trees actually inlined parts of outer loops within the inner loops (because  g atéateinmt 150 TS erate native code with nearly the same structure but better perforstring-valdate-inout (194 string-unpack-code (121 hm mance. Wlffaf:m‘dlj:x: H“__—__” Call threading, also known as context threading (8), compiles singbasess (250 [T ] S methods by generating a native call instruction to an interpreter e ?__'___ method for each inte?rpreter bytecode. A (.:all-re_turn pair has b.een math-partialsums (5.9 (Ll shown to be a potentially much more efficient dispatch mechanism e 5‘%‘ than the indirect jumps used in standard bytecode interpreters. dateformattofte (11 [T Inline threading (15) copies chunks of interpreter native code Tl which implement the required bytecodes into a native code cache, crvptoses (160 T [N 1] thus acting as a simple per-method JIT compiler that eliminates the controlflow-recursive (1ox) [ — 1] : e dispatch overhead. bitops-bitwise-and (25.20 [ ST} Neither call threading nor inline threading perform type specialstape s o I Lo ‘ < &P ype sp bitops-aitbitsin-byte (255x) | ization. . ) . o accessnsieve (50x) T I Apple’s SquirrelFish Extreme (5) is a JavaScript implementasccess bocy (4.2 IS [ S . o . o L e o sccese et (222 T tion based on call threading with selective inline threading. Comsccessebinantress (09 [ 1| bined with efficient interpreter engineering, these threading techsyt (120 [ [ — . . . st 250 L niques have given SFX excellent performance on the standard Sunsdcube (224) _‘—Spider benchmarks. 0% 20% 40% 60% 80% 100% Google’s V8 is a JavaScript implementation primarily based ! on inline threading, with call threading only for very complex Ointerpret  @Monitor BRecord ECompile  ®CallTrace  BRun Trace Uper‘dtiUnSA -_ 9. Conclusions Figure 12. Fraction of time spent on major VM activities. The . . i o speedup vs. interpreter is shown in parentheses next to each test. This paper described how to run _dy"‘”""' lang_uages ethc.lently by Most programs where the VM spends the majority of its time runrecording _hot tr}lces and generaupg ty[.)e-:%pecmllzed nanv-e code. ning native code have a good speedup. Recording and compilation Our technique focuses on aggressively inlined loops, and for each costs can be substantial; speeding up those parts of the implemenloop, it generates a tree of native code traces representing the tation would improve SunSpider performance. pdths_and value ty_pes t.k{luugh the lf)up ubse.:rved.at run time. We explained how to identify loop nesting relationships and generate nested traces in order to avoid excessive code duplication due to the many paths through a loop nest. We described our type specialization algorithm. We also described our trace compiler, inner loops become hot first), leading to much greater tail duplicawhich translates a trace from an intermediate representation to tion. optimized native code in two linear passes. YETI, from Zaleski et al. (19) applied Dynamo-style tracing Our experimental results show that in practice loops typically to Java in order to achieve inlining, indirect jump elimination, are entered with only a few different combinations of value types and other optimizations. Their primary focus was on designing an of variables. Thus, a small number of traces per loop is sufficient interpreter that could easily be gradually re-engineered as a tracing to run a program efficiently. Our experiments also show that on VM. programs amenable to tracing, we achieve speedups of 2x to 20x. Suganuma et al. (18) described region-based compilation (RBC). a relative of tracing. A region is an subprogram worth optimizing 10. Future Work that can include subsets of any number of methods. Thus, the como . . . . . A . Work is underway in a number of areas to further improve the piler has more flexibility and can potentially generate better code, e e T e o . but the profiline and compilation systems are correspondinely more performance of our trace-based JavaScript compiler. We currently P S P SYS : Sp 2y do not trace across recursive function calls, but plan to add the complex. ) N L VS ) . S . . support for this capability in the near term. We are also exploring Type specialization for dynamic languages. Dynamic lan- . S ) Lo . v . c . adoption of the existing work on tree recompilation in the context guage implementors have long recognized the importance of type . . o L A o R . . . of the presented dynamic compiler in order to minimize JIT pause specialization for performance. Most previous work has focused on L . D e e - . . times and obtain the best of both worlds, fast tree stitching as well methods instead of traces. _ . X A e . . . o . as the improved code quality due to tree recompilation. Chambers et. al (9) pioneered the idea of compiling multiple . . . . | - - S . . ¢ ‘We also plan on adding support for tracing across regular exversions of a procedure specialized for the input types in the lan" IR - e . . . . L pression substitutions using lambda functions, function applicaguage Self. In one implementation, they generated a specialized L o . . > . N . . tions and expression evaluation using eval. All these language method online each time a method was called with new input types. e ) .S o . A o . . constructs are currently executed via interpretation, which limits In another, they used an offline whole-program static analysis to . . S S j L . . our performance for applications that use those features. infer input types and constant receiver types at call sites. Interestingly. the two techniques produced nearly the same performance. Salib (17) designed a type inference algorithm for Python based Acknowledgments on the Cartesian Product Algorithm and used the results to specialParts of this effort have been sponsored by the National Science ize on types and translate the program to C++. Foundation under grants CNS-0615443 and CNS-0627747, as well McCloskey (14) has work in progress based on a languageas by the California MICRO Program and industrial sponsor Sun independent type inference that is used to generate efficient C Microsystems under Project No. 07-127. implementations of JavaScript and Python programs. The U.S. Government is authorized to reproduce and distribute Native code generation by interpreters. The traditional interreprints for Governmental purposes notwithstanding any copyright preter design is a virtual machine that directly executes ASTs or annotation thereon. Any opinions, findings, and conclusions or recmachine-code-like bytecodes. Researchers have shown how to genommendations expressed here are those of the author and should  not be interpreted as necessarily representing the official views, [10] A. Gal. Efficient Bytecode Verification and Compilation in a Virtual policies or endorsements, either expressed or implied, of the NaMachine Dissertation. PhD thesis, University Of California, Irvine, tional Science foundation (NSF), any other agency of the U.S. Gov2006. ernment, or any of the companies mentioned above. [11] A. Gal, C. W. Probst, and M. Franz. HotpathVM: An effective JIT y p compiler for resource-constrained devices. In Proceedings of the References International Conference on Virtual Execution Environments, pages i 144-153. ACM Press, 2006. t })‘;/arf‘gm(;g?d}:?;ﬁ) 2008 - hutp:/lua-users.org/lists/lua-1/2008[12] C. Garrett, J. Dean, D. Grove, and C. Chambers. Measurement and M 'il F fb b 4 Thunderbird 1 i Application of Dynamic Receiver Class Distributions. 1994. — S e t - 2l hn(:)z';/waww m;;illzxan; rowser an underbird email clien [13] J. Ha, M. R. Haghighat, S. Cong, and K. S. McKinley. A concurrent ) . T . trace-based just-in-time compiler for javascript. Dept.of Computer [3] SPECIVMOS - http://www.spec.org/jvm98/. Sciences, The University of Texas at Austin, TR-09-06, 2009. [4] SpiderMonkey i i Ua}’“SCf‘P"C) Engine - [14] B. McCloskey. Personal communication. http://www.mozilla.org/js/spidermonkey/. ) . . o . . . L . ) . ) . [15] I Piumarta and F. Riccardi. Optimizing direct threaded code by selec[5] Surfin” Safari - Blog Archive - Announcing SquirrelFish Extreme - tive inlining. In Proceedings of the ACM SIGPLAN 1998 conference http://webkit.org/blog/214/introducing-squirrelfish-extreme/. on Programming language design and implementation, pages 291— [6] A. Aho, R. Sethi, J. Ullman, and M. Lam. Compilers: Principles, 300. ACM New York, NY, USA, 1998. techniques, and tools, 2006. [16] A. Rigo. Representation-Based Just-In-time Specialization and the [7] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: A transparent Psyco Prototype for Python. In PEPM, 2004. dynamic optimization sysl‘cm. In P"“")L’d”’éf“' of the ACM SIGPL_AN [17] M. Salib. Starkiller: A Static Type Inferencer and Compiler for Conference on Programming Language Design and Implementation, Python. In Master’s Thesis, 2004 ages 1-12. ACM Press, 2000. . o o . 3 i’IgB dl. B. Vitale. M. Zaleski. and A. B C Threadine: [18] T. Suganuma, T. Yasue, and T. Nakatani. A Region-Based Compila[8] M. erndl, B. Vitale, M. Zaleskt, and A. Brown. Context Threading: tion Technique for Dynamic Compilers. ACM Transactions on Proa Flexible and Efficient Dispatch Technique for Virtual Machine Insramming Languages and Systems (TOPLAS), 28(1):134—174, 2006, terpreters. In Code Generation and Optimization, 2005. CGO 2005. 8 § fanguages and ystems /s 8L s = International Symposium on, pages 15-26, 2005. [19] M. Zal-cskl, A. D. Brown, and K. Stoodley. YETI: A graduallY 91 C. Chamb dD. U Customization: Optimizing C i Extensible Trace Interpreter. In Proceedings of the International 191 C. Cham| crs anc L. Lngar.  fustomization: Uptimizing fompiier Conference on Virtual Execution Environments, pages 83-93. ACM Technology for SELF, a Dynamically-Typed O bject-Oriented ProPress. 2007 gramming Language. In Proceedings of the ACM SIGPLAN 1989 : Conference on Programming Language Design and Implementation, pages 146-160. ACM New York, NY, USA, 1989. 